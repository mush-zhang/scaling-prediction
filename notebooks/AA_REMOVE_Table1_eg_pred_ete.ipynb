{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f46a6e-e57c-441e-a27e-60ebaf5a9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table 1\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "from helpers.expr_data import ExprData\n",
    "from helpers.scale_data import ScaleData\n",
    "from helpers.similarity import Similarity\n",
    "from helpers.feature_selection import FeatureSelection\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error as rmse_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a1bd26-d2b1-454b-8086-3b8826cef17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(47906)\n",
    "random.seed(47906)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37fbc46-edf7-4642-9af9-75955a70df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 22\n",
    "SMALL_SMALL_SIZE = 10\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "\n",
    "plt.rc('legend', fontsize=SMALL_SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867121d9-7099-4aa8-bf74-3454e8dc4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups = ['10', '11', '12']\n",
    "candidate_group = '11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ed32e7-f78c-4c6d-a191-1bcf9cfe8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = ExprData()\n",
    "data_all.load_pickle()\n",
    "# keep tpch as it is as it provides some other behavior pattern when using different cpu number (const)\n",
    "# data_all = data_all.merge_tpch()\n",
    "\n",
    "known_data = data_all.remove_by_wlname(['xml', 'ycsb'])\n",
    "ycsb_data = data_all.remove_by_wlname(['xml', 'tpcc', 'tpch', 'twitter'])\n",
    "\n",
    "simi_data = data_all.remove_by_wlname(['xml'])\n",
    "simi_data = simi_data.remove_by_group([g for g in all_groups if g != candidate_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9adbd2b-852e-4c0a-bb8b-44f995dd8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycsb_data = ycsb_data.remove_by_group([g for g in all_groups if g != candidate_group])\n",
    "ycsb_data = ycsb_data.sample_data()\n",
    "X_label = 'cpu_num'\n",
    "y_label = 'throughput'\n",
    "\n",
    "expr_label = 'EXPR'\n",
    "y_true_label = 'Y_TRUE'\n",
    "y_pred_label = 'Y_PRED'\n",
    "suffix_labels = ['_small', '_large']\n",
    "sampled_data = known_data.sample_data()\n",
    "data_by_type = sampled_data.split_by_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe571f1a-2bc8-48a1-a05c-126212213219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity computation using known data and experiment data\n",
    "scaler = ScaleData()\n",
    "plan_mtxs, plan_col_ranges = scaler.scale(simi_data.plan_mtxs)\n",
    "perf_mtxs, perf_col_ranges = scaler.scale(simi_data.perf_mtxs)\n",
    "\n",
    "simi_calc = Similarity(simi_data, plan_mtxs, plan_col_ranges, perf_mtxs, perf_col_ranges)\n",
    "simi_calc.calc_bined_mtx() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f8b9999-566d-47e6-880f-12809b798076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simis(simi_data, simi_mtx):\n",
    "    print(simi_data.cpu_nums[candid_idx])\n",
    "    arr = []\n",
    "    true_idx = []\n",
    "    for idx, val in enumerate(simi_mtx[candid_idx][:-12]):\n",
    "        if simi_data.cpu_nums[idx] == simi_data.cpu_nums[candid_idx]:\n",
    "            arr.append(val)\n",
    "            true_idx.append(idx)\n",
    "   \n",
    "    nearest = np.argsort(arr)\n",
    "    nearest_wls = np.array([simi_data.wl_groups[true_idx[i]] for i in nearest])\n",
    "    _, idx = np.unique(nearest_wls, return_index=True)\n",
    "    tops = nearest_wls[np.sort(idx)]\n",
    "    return tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b89f9b-1704-46c8-8eff-b32b7e2c49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse_score(y_true, y_pred):\n",
    "    return rmse_score(y_true, y_pred)/(np.max(y_true)-np.min(y_true))\n",
    "\n",
    "score_func = make_scorer(nrmse_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7a1f14-2670-4609-beac-95cc7c4389d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_nums_as_X(l):\n",
    "    return np.array([int(e[3:]) for e in l]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94269c8f-e816-4eec-8eda-3ab91a260ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use a dictionary of models\n",
    "- key: (lower SKU, higher SKU)\n",
    "- value: model\n",
    "'''\n",
    "def predict(model_dicts, X, y_true, expr_idxs, method, show_fig=True):\n",
    "    overall_data = pd.DataFrame(zip(X, expr_idxs, y_true), columns=[X_label, expr_label, y_true_label])\n",
    "\n",
    "    # for each pair of SKU\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    test_scores = []\n",
    "    models, datas_big, datas_small = [], [], []\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            \n",
    "            curr_smaller = overall_data[overall_data[X_label] == cpu_a]\n",
    "            curr_bigger = overall_data[overall_data[X_label] == cpu_b]\n",
    "            \n",
    "            new_preds = []\n",
    "            \n",
    "            for model_dict in model_dicts:            \n",
    "                new_y_true, new_y_pred = [], []\n",
    "\n",
    "                curr_smaller_pred = model_dict[(cpu_a, cpu_b)].predict(curr_smaller[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_bigger_pred = model_dict[(cpu_a, cpu_b)].predict(curr_bigger[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_smaller = curr_smaller.assign(Y_PRED=curr_smaller_pred)\n",
    "                curr_bigger = curr_bigger.assign(Y_PRED=curr_bigger_pred)\n",
    "\n",
    "                for _, smaller_row in curr_smaller.iterrows():\n",
    "                    curr_expr_idx = smaller_row[expr_label]\n",
    "                    curr_diff = smaller_row[y_true_label] - smaller_row[y_pred_label]\n",
    "                    bigger_row = curr_bigger[curr_bigger[expr_label] == curr_expr_idx]\n",
    "                    assert(bigger_row.shape[0] == 1)\n",
    "                    bigger_row = bigger_row.iloc[0]\n",
    "                    new_y_true.append(bigger_row[y_true_label])\n",
    "                    new_y_pred.append(bigger_row[y_pred_label] + curr_diff)\n",
    "                new_preds.append(new_y_pred)\n",
    "            \n",
    "            curr_bigger = curr_bigger.assign(Y_PRED=np.mean(np.array(new_preds), axis=0))\n",
    "            df = pd.merge(curr_smaller, curr_bigger, on=[expr_label], suffixes=suffix_labels)\n",
    "            dfs.append(df)\n",
    "            \n",
    "            new_y_true = np.array(new_y_true)\n",
    "            new_y_pred = np.array(new_y_pred) \n",
    "            \n",
    "            score = rmse_score(new_y_true, new_y_pred)/(np.max(new_y_true) - np.min(new_y_true))\n",
    "            test_scores.append(score)\n",
    "                \n",
    "            models.append(model_dict[(cpu_a, cpu_b)])\n",
    "            datas_small.append(curr_smaller)\n",
    "            datas_big.append(curr_bigger)\n",
    "\n",
    "    overall_score = np.mean(test_scores)\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bdad28c5-a604-4338-8427-aae2e32c6532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use a dictionary of models\n",
    "- key: (lower SKU, higher SKU)\n",
    "- value: model\n",
    "'''\n",
    "def predict_pair(model_dicts, sku_pair, X, y_true, expr_idxs, method, plot=True, show_fig=True):\n",
    "    overall_data = pd.DataFrame(zip(X, expr_idxs, y_true), columns=[X_label, expr_label, y_true_label])\n",
    "\n",
    "    # for each pair of SKU\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    test_scores = []\n",
    "    models, datas_big, datas_small = [], [], []\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i != sku_pair[0] or j != sku_pair[1]:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            \n",
    "            curr_smaller = overall_data[overall_data[X_label] == cpu_a]\n",
    "            curr_bigger = overall_data[overall_data[X_label] == cpu_b]\n",
    "            \n",
    "            new_preds = []\n",
    "            \n",
    "            for model_dict in model_dicts:            \n",
    "                new_y_true, new_y_pred = [], []\n",
    "\n",
    "                curr_smaller_pred = model_dict[(cpu_a, cpu_b)].predict(curr_smaller[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_bigger_pred = model_dict[(cpu_a, cpu_b)].predict(curr_bigger[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_smaller = curr_smaller.assign(Y_PRED=curr_smaller_pred)\n",
    "                curr_bigger = curr_bigger.assign(Y_PRED=curr_bigger_pred)\n",
    "\n",
    "                for _, smaller_row in curr_smaller.iterrows():\n",
    "                    curr_expr_idx = smaller_row[expr_label]\n",
    "                    curr_diff = smaller_row[y_true_label] - smaller_row[y_pred_label]\n",
    "                    bigger_row = curr_bigger[curr_bigger[expr_label] == curr_expr_idx]\n",
    "                    assert(bigger_row.shape[0] == 1)\n",
    "                    bigger_row = bigger_row.iloc[0]\n",
    "                    new_y_true.append(bigger_row[y_true_label])\n",
    "                    new_y_pred.append(bigger_row[y_pred_label] + curr_diff)\n",
    "                new_preds.append(new_y_pred)\n",
    "            \n",
    "            curr_bigger = curr_bigger.assign(Y_PRED=np.mean(np.array(new_preds), axis=0))\n",
    "            df = pd.merge(curr_smaller, curr_bigger, on=[expr_label], suffixes=suffix_labels)\n",
    "            dfs.append(df)\n",
    "            \n",
    "            new_y_true = np.array(new_y_true)\n",
    "            new_y_pred = np.array(new_y_pred) \n",
    "            \n",
    "            score = rmse_score(new_y_true, new_y_pred)/(np.max(new_y_true) - np.min(new_y_true))\n",
    "            test_scores.append(score)\n",
    "                \n",
    "            models.append(model_dict[(cpu_a, cpu_b)])\n",
    "            datas_small.append(curr_smaller)\n",
    "            datas_big.append(curr_bigger)\n",
    "    \n",
    "    overall_score = np.mean(test_scores)\n",
    "    return overall_score, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102bc9bc-e711-46cc-b665-6832a386f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regression_model(X, y, wl_name, grouping_type, groupping_id, plot=False, show_fig=False):  \n",
    "    train_rmses, test_rmses = [], []\n",
    "    k = 5\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    train_time = 0\n",
    "    model_dict = {}\n",
    "    models = []\n",
    "    fold_test_rmses = []\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        start = time.time()\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X_train, y_train)\n",
    "        test_pred = reg.predict(X_test)\n",
    "        train_pred = reg.predict(X_train)\n",
    "        end = time.time()\n",
    "        train_time += end - start\n",
    "        \n",
    "        num_pairs = 0\n",
    "        for i in range(len(num_cpus)):\n",
    "            for j in range(i, len(num_cpus)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                cpu_a = num_cpus[i] # smaller\n",
    "                cpu_b = num_cpus[j] # larger\n",
    "                curr_mask = [x_lab == cpu_a or x_lab == cpu_b for x_lab in X_test.flatten()]\n",
    "\n",
    "                curr_y_true = y_test[curr_mask]\n",
    "                rmse = np.sqrt(((curr_y_true-test_pred[curr_mask])**2).mean())\n",
    "                n_rmse = rmse / (np.max(curr_y_true)-np.min(curr_y_true))\n",
    "                test_rmses.append(n_rmse)\n",
    "                \n",
    "                curr_mask = [x_lab == cpu_a or x_lab == cpu_b for x_lab in X_train.flatten()]\n",
    "                \n",
    "                curr_y_true = y_train[curr_mask]\n",
    "                rmse = np.sqrt(((curr_y_true-train_pred[curr_mask])**2).mean())\n",
    "                n_rmse = rmse / (np.max(curr_y_true)-np.min(curr_y_true))\n",
    "                train_rmses.append(n_rmse)\n",
    "                num_pairs == 1\n",
    "                \n",
    "        fold_test_rmses.append(np.mean(test_rmses[-num_pairs:]))\n",
    "        models.append(reg)\n",
    "    \n",
    "    train_time /= k\n",
    "    \n",
    "    best_model = models[np.argmin(fold_test_rmses)]\n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            model_dict[(cpu_a, cpu_b)] = best_model\n",
    "\n",
    "    overall_test_rmse = np.mean(test_rmses)\n",
    "    overall_train_rmse = np.mean(train_rmses)\n",
    "    return overall_test_rmse, overall_train_rmse, model_dict, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cb86cd-62e1-4a5f-94ea-022b209ac8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_svm_model_indi(X, y, wl_name, grouping_type, groupping_id, show_fig=True):  \n",
    "    metrics = [X_label, y_label]\n",
    "    zipped = zip(X, y)\n",
    "    \n",
    "    # append fixed effect and random effect to data\n",
    "    overall_data = pd.DataFrame(zipped, columns=metrics)\n",
    "\n",
    "    train_scores, test_scores = [], []\n",
    "\n",
    "    models, datas = [], []\n",
    "    model_dict = {}\n",
    "    \n",
    "    # epsilon range: according to this paper: http://adrem.uantwerpen.be/bibrem/pubs/IJCNN2007.pdf\n",
    "    # C range: https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n",
    "    \n",
    "    # Cross validation grid search (best parameters) \n",
    "    c_range = np.logspace(-5, 5, base=2.0, num=5) # 1 and 100\n",
    "    epsilon_range = np.linspace(10 ** -3, 1, 5) # 1-e3 and 1\n",
    "    degree_range = np.array(list(range(1, 5)))\n",
    "    tuned_parameters = [{'kernel': ['rbf'],'C': c_range, 'epsilon': epsilon_range}, \n",
    "                        {'kernel': ['linear'], 'C': c_range, 'epsilon': epsilon_range}, \n",
    "                        {'kernel': ['poly'], 'C': c_range, 'degree': degree_range, 'epsilon': epsilon_range} \n",
    "                       ]\n",
    "\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    prev_cpu = num_cpus[0]\n",
    "\n",
    "    train_time = 0\n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            curr_data = overall_data[ (overall_data[X_label] == cpu_a) | (overall_data[X_label] == cpu_b) ]\n",
    "            \n",
    "            svr = SVR()\n",
    "            datas.append(curr_data)\n",
    "            clf = GridSearchCV(svr,param_grid=tuned_parameters,verbose=0, n_jobs=4,\n",
    "                               cv=5, \n",
    "                               scoring=score_func,\n",
    "                               return_train_score=True)\n",
    "            clf.fit(curr_data[[X_label]], curr_data[y_label])           \n",
    "            best_params = clf.best_params_\n",
    "            results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "            results_best = results[results['params'] == best_params].reset_index()\n",
    "            train_time += results_best['mean_fit_time'][0]\n",
    "\n",
    "            test_scores.append(results_best.iloc[0]['mean_test_score'])\n",
    "            train_scores.append(results_best.iloc[0]['mean_train_score'])\n",
    "            models.append(clf)\n",
    "            model_dict[(cpu_a, cpu_b)] = clf\n",
    "    \n",
    "    # plot the last one as example\n",
    "    overall_test = np.mean(test_scores)\n",
    "    overall_train = np.mean(train_scores)\n",
    "    return overall_test, overall_train, model_dict, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5944a18a-b0b6-452f-a7e3-f8de3de6b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gb_model_indi(X, y, wl_name, grouping_type, groupping_id, plot=False, show_fig=True):  \n",
    "    metrics = [X_label, y_label]\n",
    "    zipped = zip(X, y)\n",
    "    \n",
    "    # append fixed effect and random effect to data\n",
    "    overall_data = pd.DataFrame(zipped, columns=metrics)\n",
    "\n",
    "    train_scores, test_scores = [], []\n",
    "\n",
    "    models, datas = [], []\n",
    "    model_dict = {}\n",
    "\n",
    "    tuned_parameters = {\n",
    "        \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "        \"max_depth\":[3,5,8],\n",
    "        \"max_features\":[\"log2\",\"sqrt\"],\n",
    "        \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "        \"n_estimators\":[10, 50, 100]\n",
    "    }\n",
    "\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    prev_cpu = num_cpus[0]\n",
    "    \n",
    "    train_time = 0\n",
    "\n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            curr_data = overall_data[ (overall_data[X_label] == cpu_a) | (overall_data[X_label] == cpu_b) ]\n",
    "            \n",
    "            datas.append(curr_data)\n",
    "            model = GradientBoostingRegressor()\n",
    "            clf = GridSearchCV(model, param_grid=tuned_parameters,verbose=0, n_jobs=-1,\n",
    "                               cv=5, \n",
    "                               scoring=score_func,  \n",
    "                               return_train_score=True)\n",
    "            clf.fit(curr_data[[X_label]], curr_data[y_label])\n",
    "\n",
    "            best_params = clf.best_params_\n",
    "            results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "            results_best = results[results['params'] == best_params].reset_index()\n",
    "            train_time += results_best['mean_fit_time'][0]\n",
    "            \n",
    "            test_scores.append(results_best.iloc[0]['mean_test_score'])\n",
    "            train_scores.append(results_best.iloc[0]['mean_train_score'])\n",
    "            models.append(clf)\n",
    "            model_dict[(cpu_a, cpu_b)] = clf\n",
    "\n",
    "    overall_test = np.mean(test_scores)\n",
    "    overall_train = np.mean(train_scores)\n",
    "    return overall_test, overall_train, model_dict, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd44556f-8ab0-4c44-9bfc-3ef9e9a86774",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycsb_X = get_cpu_nums_as_X(ycsb_data.cpu_nums)\n",
    "ycsb_y = np.array(ycsb_data.wl_throughput)\n",
    "ycsb_expr = np.array(ycsb_data.sampled_run_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b75d115-7049-4f7a-ad65-3fc8d17e8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "candid_idx = len(simi_data.wl_names) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c41b2b88-edae-4454-b2bd-07c9fe96a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_and_pred(group_to_model_dict, tops, k, name):\n",
    "    min_groups = tops[:k]\n",
    "    model_dict = [group_to_model_dict[min_group] for min_group in min_groups]\n",
    "    print(min_groups)\n",
    "    overall_score = predict(model_dict, ycsb_X, ycsb_y, ycsb_expr, name, False)\n",
    "\n",
    "    print('{} Overall ycsb nrmse: {}'.format(name, overall_score))\n",
    "    for p in [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3)]:\n",
    "        pair_score, _ = predict_pair(model_dict, p, ycsb_X, ycsb_y, ycsb_expr, f'name-{p[0]}-{p[1]}', False)\n",
    "        print('{} pairwise ycsb nrmse: {}'.format(p, pair_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47948c17-72e6-4b1a-adc2-8e769f64f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using know data\n",
    "kscaler = ScaleData()\n",
    "plan_mtxs, plan_col_ranges = scaler.scale(known_data.plan_mtxs)\n",
    "perf_mtxs, perf_col_ranges = scaler.scale(known_data.perf_mtxs)\n",
    "\n",
    "ksimi_calc = Similarity(known_data, plan_mtxs, plan_col_ranges, perf_mtxs, perf_col_ranges)\n",
    "ksimi_calc.calc_bined_mtx() # all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36592813-8906-4e09-9a80-768be25278fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureSelection(ksimi_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87093d-7e15-4b31-8baa-6ffceab6799b",
   "metadata": {},
   "source": [
    "# Strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddef7e5-bfab-4af0-9df4-61f84193dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_7 = fs.select_features(7, 'Lasso', est_name=None, direction=None, feature_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6faf78b-a8f4-44b8-a667-81615a0cf15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simi calculation time 0.15011119842529297\n"
     ]
    }
   ],
   "source": [
    "simi_calc.calc_dist_simi_matrix(cumulative=True, feature_names=top_7, norm_type='l21', timeit=True)\n",
    "simi_mtx = simi_calc.simi_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b21ae232-f93c-481a-9177-84273fc7f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu8\n"
     ]
    }
   ],
   "source": [
    "tops = get_simis(simi_data, simi_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9af45189-fb54-44f9-ba5e-31b421d07eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8' '9' '7' '3' '4' '1' '2' '6' '5']\n",
      "['tpcc'] cpu8 [4] ['8']\n",
      "['twitter'] cpu8 [4] ['9']\n",
      "['tpcc'] cpu8 [8] ['7']\n",
      "['twitter'] cpu8 [32] ['3']\n",
      "['twitter'] cpu8 [8] ['4']\n",
      "['tpcc'] cpu8 [32] ['1']\n",
      "['tpch'] cpu8 [32] ['2']\n",
      "['tpch'] cpu8 [4] ['6']\n",
      "['tpch'] cpu8 [8] ['5']\n"
     ]
    }
   ],
   "source": [
    "print(tops)\n",
    "for wl in tops:\n",
    "    min_idx, = np.where(np.array(simi_data.wl_groups) == wl)\n",
    "    print(np.unique(np.array(simi_data.wl_names)[min_idx]), simi_data.cpu_nums[candid_idx], \n",
    "          np.unique(np.array(simi_data.terminal_num)[min_idx]), \n",
    "          np.unique(np.array(simi_data.wl_groups)[min_idx])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edd3ceec-2a67-462d-9ef0-231a32d768e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ycsb cpu8 8 11\n"
     ]
    }
   ],
   "source": [
    "print(simi_data.wl_names[candid_idx], simi_data.cpu_nums[candid_idx], \n",
    "      simi_data.terminal_num[candid_idx], \n",
    "      simi_data.wl_groups[candid_idx]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef248-b39a-4543-ac7b-1a7fec1f69ad",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f533178-da70-4efa-a25d-49e83298e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0b54596-18e2-4f2f-89d3-66ea1de9ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test nrmse: -0.2984995299208955, train -0.2984995299208955\n"
     ]
    }
   ],
   "source": [
    "all_tests, all_trains = [], []\n",
    "svm_model_dict = {}\n",
    "svm_results = {}\n",
    "\n",
    "for ty, curr_data in data_by_type.items():\n",
    "    if ty not in tops:\n",
    "        continue\n",
    "    X = get_cpu_nums_as_X(curr_data.cpu_nums)\n",
    "    y = np.array(curr_data.wl_throughput)\n",
    "    test_r2_mean, train_r2_mean, model_dict, train_time = build_svm_model_indi(X, y, curr_data.wl_names[0], 'group', \n",
    "                                                                   curr_data.wl_groups[0], show_fig=False)\n",
    "    svm_model_dict[ty] = model_dict\n",
    "    all_tests.append(test_r2_mean)\n",
    "    all_trains.append(train_r2_mean)\n",
    "    svm_results[ty] = test_r2_mean\n",
    "\n",
    "print('Overall test nrmse: {}, train {}'.format(np.mean(all_tests), np.mean(all_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc59b550-d5bb-409a-8955-c9aee150913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8']\n",
      "svm Overall ycsb nrmse: 0.17796459071596096\n",
      "(0, 1) pairwise ycsb nrmse: 0.05787888360721371\n",
      "(0, 2) pairwise ycsb nrmse: 0.1114068367745914\n",
      "(1, 2) pairwise ycsb nrmse: 0.10456502811028062\n",
      "(1, 3) pairwise ycsb nrmse: 0.2717916007760527\n",
      "(2, 3) pairwise ycsb nrmse: 0.21350769960387353\n",
      "['8' '9']\n",
      "svm Overall ycsb nrmse: 0.1685264898283195\n",
      "(0, 1) pairwise ycsb nrmse: 0.06339904392900461\n",
      "(0, 2) pairwise ycsb nrmse: 0.1023289618827474\n",
      "(1, 2) pairwise ycsb nrmse: 0.07532017623081813\n",
      "(1, 3) pairwise ycsb nrmse: 0.25105570037101366\n",
      "(2, 3) pairwise ycsb nrmse: 0.21450938407206369\n",
      "['8' '9' '7']\n",
      "svm Overall ycsb nrmse: 0.17649689519008852\n",
      "(0, 1) pairwise ycsb nrmse: 0.05879547563886495\n",
      "(0, 2) pairwise ycsb nrmse: 0.10936289450460114\n",
      "(1, 2) pairwise ycsb nrmse: 0.10270361865882784\n",
      "(1, 3) pairwise ycsb nrmse: 0.2688365830532106\n",
      "(2, 3) pairwise ycsb nrmse: 0.21353628494357715\n"
     ]
    }
   ],
   "source": [
    "for k in [1 ,2, 3]:\n",
    "    chose_and_pred(svm_model_dict, tops, k, 'svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e692cb5-d3e3-4e17-a85c-a8d65562201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8' '9']\n",
      "   cpu_num_small  EXPR  Y_TRUE_small  Y_PRED_small cpu_num_large  \\\n",
      "17           [4]    17   1325.623955   2895.011111          [16]   \n",
      "\n",
      "    Y_TRUE_large  Y_PRED_large      Diff  \n",
      "17   1368.144444   1368.875346  0.730902  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_TRUE_small</th>\n",
       "      <th>Y_TRUE_large</th>\n",
       "      <th>Y_PRED_large</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1014.152739</td>\n",
       "      <td>1081.140000</td>\n",
       "      <td>1057.404130</td>\n",
       "      <td>111.010977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.040368</td>\n",
       "      <td>215.261745</td>\n",
       "      <td>266.040368</td>\n",
       "      <td>76.957344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>664.172702</td>\n",
       "      <td>897.738889</td>\n",
       "      <td>707.424093</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>693.075209</td>\n",
       "      <td>922.231944</td>\n",
       "      <td>736.326599</td>\n",
       "      <td>28.859926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1035.087744</td>\n",
       "      <td>946.151389</td>\n",
       "      <td>1078.339134</td>\n",
       "      <td>144.595189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1308.350279</td>\n",
       "      <td>1350.628472</td>\n",
       "      <td>1351.601669</td>\n",
       "      <td>178.799850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1347.824513</td>\n",
       "      <td>1425.416667</td>\n",
       "      <td>1391.075903</td>\n",
       "      <td>224.267574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y_TRUE_small  Y_TRUE_large  Y_PRED_large        Diff\n",
       "count     30.000000     30.000000     30.000000   30.000000\n",
       "mean    1014.152739   1081.140000   1057.404130  111.010977\n",
       "std      266.040368    215.261745    266.040368   76.957344\n",
       "min      664.172702    897.738889    707.424093    0.730902\n",
       "25%      693.075209    922.231944    736.326599   28.859926\n",
       "50%     1035.087744    946.151389   1078.339134  144.595189\n",
       "75%     1308.350279   1350.628472   1351.601669  178.799850\n",
       "max     1347.824513   1425.416667   1391.075903  224.267574"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_groups = tops[:2]\n",
    "model_dict = [svm_model_dict[min_group] for min_group in min_groups]\n",
    "print(min_groups)\n",
    "\n",
    "pair_score, dfs = predict_pair(model_dict, (1, 3), ycsb_X, ycsb_y, ycsb_expr, f'svm13', False)\n",
    "curr_df = dfs[0]\n",
    "curr_df['Diff'] = abs(curr_df['Y_TRUE_large']-curr_df['Y_PRED_large'])\n",
    "print(curr_df[curr_df['Diff'] == curr_df['Diff'].min()])\n",
    "temp = curr_df[['Y_TRUE_small', 'Y_TRUE_large', 'Y_PRED_large', 'Diff']]\n",
    "temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d52bae-8d96-4140-b45e-62f17d1f1326",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9cd5a-f35a-4141-8fac-6185c84d75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3 = fs.select_features(3, 'Chi2', est_name=None, direction=None, feature_type='perf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c255fe3-8115-478f-862d-e7bb615263aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "simi_calc.calc_simi_matrix(perf_feature_names=top_3,  norm_type='corr')\n",
    "\n",
    "simi_mtx = simi_calc.simi_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "51d2a4e7-4e10-42d2-86d7-fed0d74ea9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu8\n"
     ]
    }
   ],
   "source": [
    "tops = get_simis(simi_data, simi_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d6af20c2-4d11-443c-b8e0-31594c6fe238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7' '1' '4' '3' '8' '2' '5' '6' '9']\n",
      "['tpcc'] cpu8 [8] ['7']\n",
      "['tpcc'] cpu8 [32] ['1']\n",
      "['twitter'] cpu8 [8] ['4']\n",
      "['twitter'] cpu8 [32] ['3']\n",
      "['tpcc'] cpu8 [4] ['8']\n",
      "['tpch'] cpu8 [32] ['2']\n",
      "['tpch'] cpu8 [8] ['5']\n",
      "['tpch'] cpu8 [4] ['6']\n",
      "['twitter'] cpu8 [4] ['9']\n"
     ]
    }
   ],
   "source": [
    "print(tops)\n",
    "for wl in tops:\n",
    "    min_idx, = np.where(np.array(simi_data.wl_groups) == wl)\n",
    "    print(np.unique(np.array(simi_data.wl_names)[min_idx]), simi_data.cpu_nums[candid_idx], \n",
    "          np.unique(np.array(simi_data.terminal_num)[min_idx]), \n",
    "          np.unique(np.array(simi_data.wl_groups)[min_idx])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5239bfd-88ed-4786-8d0c-54393d9976ea",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ad65ee0-ffcc-43b0-9c71-af93bee20f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d62e767-d8a4-4d42-946a-c449b5e2e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test nrmse: 0.3308316949489202, train 0.3308316949489202\n"
     ]
    }
   ],
   "source": [
    "all_tests, all_trains = [], []\n",
    "reg_model_dict = {}\n",
    "reg_results = {}\n",
    "\n",
    "for ty, curr_data in data_by_type.items():\n",
    "    if ty not in tops:\n",
    "        continue\n",
    "    X = get_cpu_nums_as_X(curr_data.cpu_nums)\n",
    "    y = np.array(curr_data.wl_throughput)\n",
    "    test_r2_mean, train_r2_mean, model_dict, train_time = build_regression_model(X, y, curr_data.wl_names[0], 'group', \n",
    "                                                                   curr_data.wl_groups[0], show_fig=False)\n",
    "    reg_model_dict[ty] = model_dict\n",
    "    all_tests.append(test_r2_mean)\n",
    "    all_trains.append(train_r2_mean)\n",
    "    reg_results[ty] = test_r2_mean\n",
    "\n",
    "print('Overall test nrmse: {}, train {}'.format(np.mean(all_tests), np.mean(all_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c4fa2b6c-6710-48bc-96a8-6bd93f5de8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7']\n",
      "reg Overall ycsb nrmse: 0.177388704527762\n",
      "(0, 1) pairwise ycsb nrmse: 0.062036508812508676\n",
      "(0, 2) pairwise ycsb nrmse: 0.10801164379654787\n",
      "(1, 2) pairwise ycsb nrmse: 0.10375571023927778\n",
      "(1, 3) pairwise ycsb nrmse: 0.2717728286714124\n",
      "(2, 3) pairwise ycsb nrmse: 0.2132882158319074\n",
      "['7' '1']\n",
      "reg Overall ycsb nrmse: 0.17529740963386412\n",
      "(0, 1) pairwise ycsb nrmse: 0.063039540038026\n",
      "(0, 2) pairwise ycsb nrmse: 0.10596977836244141\n",
      "(1, 2) pairwise ycsb nrmse: 0.10112366128523766\n",
      "(1, 3) pairwise ycsb nrmse: 0.2663461127672552\n",
      "(2, 3) pairwise ycsb nrmse: 0.21301835819851778\n",
      "['7' '1' '4']\n",
      "reg Overall ycsb nrmse: 0.19437631023008675\n",
      "(0, 1) pairwise ycsb nrmse: 0.05760472378155589\n",
      "(0, 2) pairwise ycsb nrmse: 0.1223464330070565\n",
      "(1, 2) pairwise ycsb nrmse: 0.11838369316398559\n",
      "(1, 3) pairwise ycsb nrmse: 0.30991269493798834\n",
      "(2, 3) pairwise ycsb nrmse: 0.22191028297502186\n"
     ]
    }
   ],
   "source": [
    "for k in [1 ,2, 3]:\n",
    "    chose_and_pred(reg_model_dict, tops, k, 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "896e5c01-e8cc-448b-b073-0b5be4ff2385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7' '1' '4']\n",
      "  cpu_num_small  EXPR  Y_TRUE_small  Y_PRED_small cpu_num_large  Y_TRUE_large  \\\n",
      "8           [4]     8    1311.16156   4676.772722          [16]   1328.644444   \n",
      "\n",
      "   Y_PRED_large       Diff  \n",
      "8    1318.06619  10.578254  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_TRUE_small</th>\n",
       "      <th>Y_TRUE_large</th>\n",
       "      <th>Y_PRED_large</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1014.152739</td>\n",
       "      <td>1081.140000</td>\n",
       "      <td>1021.057370</td>\n",
       "      <td>121.301543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.040368</td>\n",
       "      <td>215.261745</td>\n",
       "      <td>266.040368</td>\n",
       "      <td>81.246254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>664.172702</td>\n",
       "      <td>897.738889</td>\n",
       "      <td>671.077332</td>\n",
       "      <td>10.578254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>693.075209</td>\n",
       "      <td>922.231944</td>\n",
       "      <td>699.979839</td>\n",
       "      <td>48.149879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1035.087744</td>\n",
       "      <td>946.151389</td>\n",
       "      <td>1041.992374</td>\n",
       "      <td>108.248429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1308.350279</td>\n",
       "      <td>1350.628472</td>\n",
       "      <td>1315.254909</td>\n",
       "      <td>204.665537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1347.824513</td>\n",
       "      <td>1425.416667</td>\n",
       "      <td>1354.729143</td>\n",
       "      <td>260.614334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y_TRUE_small  Y_TRUE_large  Y_PRED_large        Diff\n",
       "count     30.000000     30.000000     30.000000   30.000000\n",
       "mean    1014.152739   1081.140000   1021.057370  121.301543\n",
       "std      266.040368    215.261745    266.040368   81.246254\n",
       "min      664.172702    897.738889    671.077332   10.578254\n",
       "25%      693.075209    922.231944    699.979839   48.149879\n",
       "50%     1035.087744    946.151389   1041.992374  108.248429\n",
       "75%     1308.350279   1350.628472   1315.254909  204.665537\n",
       "max     1347.824513   1425.416667   1354.729143  260.614334"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_groups = tops[:3]\n",
    "model_dict = [svm_model_dict[min_group] for min_group in min_groups]\n",
    "print(min_groups)\n",
    "\n",
    "pair_score, dfs = predict_pair(model_dict, (1, 3), ycsb_X, ycsb_y, ycsb_expr, f'svm13', False)\n",
    "curr_df = dfs[0]\n",
    "curr_df['Diff'] = abs(curr_df['Y_TRUE_large']-curr_df['Y_PRED_large'])\n",
    "print(curr_df[curr_df['Diff'] == curr_df['Diff'].min()])\n",
    "temp = curr_df[['Y_TRUE_small', 'Y_TRUE_large', 'Y_PRED_large', 'Diff']]\n",
    "temp.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workload_insights",
   "language": "python",
   "name": "workload_insights"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
