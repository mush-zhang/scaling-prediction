{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b040608c-b6b2-484a-97ec-0d2c3ae19811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "from helpers.expr_data import ExprData\n",
    "from helpers.scale_data import ScaleData\n",
    "from helpers.similarity import Similarity\n",
    "from helpers.feature_selection import FeatureSelection\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import root_mean_squared_error as rmse_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d594928-08fc-4400-9e2a-53dbdfbc941c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 22\n",
    "SMALL_SMALL_SIZE = 10\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "\n",
    "# plt.rc('legend', fontsize=SMALL_SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfa50a2-27bc-4d9e-bac2-fd57b5e49124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81476861-84a8-48ab-a27b-94cb2ec900d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_PLOT = False\n",
    "# for measuring modeling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affb933d-d824-40f1-8b2d-28048f580cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse_score(y_true, y_pred):\n",
    "    # return np.sqrt((((y_true-y_pred)/y_pred)**2).mean())    \n",
    "    # return np.sqrt(((abs(y_true-y_pred)/y_pred)).mean())    \n",
    "    return rmse_score(y_true, y_pred)/(np.max(y_true)-np.min(y_true))\n",
    "    # return rmse_score(y_true, y_pred)/(np.mean(y_true))\n",
    "\n",
    "score_func = make_scorer(nrmse_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b597f4-f58e-480c-9856-b48fa0de744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the performance metrics for each experiment\n",
    "data_all = ExprData()\n",
    "data_all.load_pickle()\n",
    "data_all = data_all.merge_tpch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8deacf-a0d5-4277-a8ae-2514e8276426",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_all.remove_by_wlname(['xml', 'ycsb'])\n",
    "ycsb_data = data_all.remove_by_wlname(['xml', 'tpcc', 'tpch', 'twitter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2670a1-ef37-4336-b0ef-3189c99ce691",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups = ['10', '11', '12']\n",
    "candidate_group = '10'\n",
    "ycsb_data = ycsb_data.remove_by_group([g for g in all_groups if g != candidate_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8917a89-a4cb-4143-90ee-179a903a6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label = 'SKU'\n",
    "expr_label = 'EXPR'\n",
    "y_true_label = 'Y_TRUE'\n",
    "y_pred_label = 'Y_PRED'\n",
    "suffix_labels = ['_small', '_large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74f5e95-30ac-42ce-b1d8-6310c9b7a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity for all\n",
    "new_data = data.keep_complete_exprs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df02f90-bcd6-4342-9a00-cfce693deb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = new_data.sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71e5e309-a1d7-4819-897b-4ce9d1c27e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_type = sampled_data.split_by_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ec1e18-1336-4752-9197-e588b9cc17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_by_type = sampled_data.split_by_type()\n",
    "for ty, expr_set in sampled_by_type.items():\n",
    "    sub_by_term = expr_set.split_by_term()\n",
    "    sampled_by_type[ty] = sub_by_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda868aa-4f7c-49e7-830e-9e92d05d5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_mtx(mtx):\n",
    "    return (mtx.T).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86f50be2-f21d-46ce-b20b-65eed7e96283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_X(curr_data):\n",
    "    scaler = ScaleData()\n",
    "    plan_mtxs, plan_col_ranges = scaler.scale(curr_data.plan_mtxs)\n",
    "    perf_mtxs, perf_col_ranges = scaler.scale(curr_data.perf_mtxs)\n",
    "\n",
    "    simi_calc = Similarity(curr_data, plan_mtxs, plan_col_ranges, perf_mtxs, perf_col_ranges)\n",
    "    simi_calc.calc_bined_mtx(plan_only=True) # all features\n",
    "\n",
    "    keep_cols = simi_calc.filter_by_features(['MaxCompileMemory', 'CachedPlanSize', 'AvgRowSize', 'StatementSubTreeCost', 'SerialRequiredMemory', 'EstimatedPagesCached', 'CompileMemory'])\n",
    "    ndarrs = [b[:, keep_cols].astype(float) for b in simi_calc.cumulative_bined]\n",
    "    print(len(ndarrs))\n",
    "    X = []\n",
    "    for i in range(len(curr_data.cpu_nums)):\n",
    "        X.append(np.insert(ndarrs[i], 0, int(curr_data.cpu_nums[i][3:])))\n",
    "    # print(len(X), len(X[0]), X[0])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2556e04c-c6ce-4ed9-a43d-3c6fd1950196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(dfs, X, Y, expr_idxs, method, plot=True, figsize=(3.6,2.4)):\n",
    "    # plot all trends \n",
    "    colors = ['#1b9e77','#7570b3','#d95f02','#e7298a']\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "    pred_label_add = False\n",
    "    pred_color = colors[0]\n",
    "    overall_expr_label_add = False\n",
    "    expr_tracker = 0\n",
    "    \n",
    "    all_true_tracker = {}\n",
    "        \n",
    "    color_map = [colors[1+idx%3] for idx, _ in enumerate(expr_idxs)]\n",
    "    ax.scatter(X, Y, color=color_map)\n",
    "    \n",
    "    for df in dfs:\n",
    "        expr_c = 1\n",
    "        expr_label_add = False\n",
    "        for _, row in df.iterrows():\n",
    "            run_color = colors[1+expr_tracker%3]\n",
    "            curr_x = [row[f'{X_label}{suffix}'] for suffix in suffix_labels]\n",
    "            expr_tracker += 1\n",
    "            true_y = [row[f'{y_true_label}{suffix}'] for suffix in suffix_labels]\n",
    "            \n",
    "            for x, y in zip(np.array(curr_x).flatten(), true_y):\n",
    "                if x not in all_true_tracker:\n",
    "                    all_true_tracker[x] = []\n",
    "                all_true_tracker[x].append(y)\n",
    "                \n",
    "            if not overall_expr_label_add:\n",
    "                if not expr_label_add and expr_tracker % 3 == 2:\n",
    "                    expr = ax.plot(curr_x, true_y, '-', color=run_color, alpha=0.5, linewidth=1.5, label=f'Expr{expr_c} True_y')\n",
    "                    expr_label_add = True\n",
    "                else:\n",
    "                    ax.plot(curr_x, true_y, '-', color=run_color, alpha=0.5, linewidth=1.5, label=f'Expr{expr_c} True_y')\n",
    "            else:\n",
    "                ax.plot(curr_x, true_y, '-', color=run_color, alpha=0.5, linewidth=1.5)\n",
    "            expr_c += 1\n",
    "        overall_expr_label_add = True\n",
    "        \n",
    "    for df in dfs:\n",
    "        for _, row in df.iterrows():\n",
    "            curr_x = [row[f'{X_label}{suffix}'] for suffix in suffix_labels]\n",
    "            pred_y = [row[f'{y_true_label}{suffix_labels[0]}'], row[f'{y_pred_label}{suffix_labels[1]}']]\n",
    "            \n",
    "            if not pred_label_add:\n",
    "                pred_label_add = True\n",
    "                pred_l = ax.plot(curr_x, pred_y, '-o', color=pred_color, linewidth=1, label='Predict_y')\n",
    "            else:\n",
    "                ax.plot(curr_x, pred_y, '-o', color=pred_color, linewidth=1)\n",
    "    \n",
    "    true_x = np.sort(list(all_true_tracker.keys()))\n",
    "    mean_true_y = [np.mean(all_true_tracker[key]) for key in true_x]\n",
    "    ax.plot(true_x, mean_true_y, '-', color='#fc8d62', linewidth=15, alpha=0.3, label='Mean_True_y')\n",
    "    max_x = np.max(true_x)\n",
    "    # ax.set_xlim(right=max_x)\n",
    "    base_ticks= [2, 4, 8, 16]\n",
    "    ax.set_xticks([xtick for xtick in base_ticks if xtick <=max_x])\n",
    "    ax.margins(0.1)           # Default margin is 0.05, value 0 means fit\n",
    "                \n",
    "    ax.set_xlabel('Num CPU')\n",
    "    ax.set_ylabel('Latency')\n",
    "    \n",
    "    ax.axis('tight')\n",
    "    plt.legend(bbox_to_anchor=(1, 0.1, 1, 1), loc='upper left', ncol=1)#, mode=\"expand\")\n",
    "\n",
    "    plt.savefig(f'figs/prediction/ycs_pred_{method}.pdf', bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240bdad1-7cc2-4e06-86b8-4b36e9daf519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use a dictionary of models\n",
    "- key: (lower SKU, higher SKU)\n",
    "- value: model\n",
    "'''\n",
    "def predict(model_dicts, X, y_true, expr_idxs, method, plot=True):\n",
    "    overall_data = pd.DataFrame(zip(expr_idxs, y_true), columns=[expr_label, y_true_label])\n",
    "    overall_data[X_label] = X\n",
    "\n",
    "    # for each pair of SKU\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    test_scores = []\n",
    "    models, datas_big, datas_small = [], [], []\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            \n",
    "            curr_smaller = overall_data[overall_data[X_label] == cpu_a]\n",
    "            curr_bigger = overall_data[overall_data[X_label] == cpu_b]\n",
    "            \n",
    "            new_preds = []\n",
    "            \n",
    "            for model_dict in model_dicts:            \n",
    "                new_y_true, new_y_pred = [], []\n",
    "\n",
    "                curr_smaller_pred = model_dict[(cpu_a, cpu_b)].predict(curr_smaller[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_bigger_pred = model_dict[(cpu_a, cpu_b)].predict(curr_bigger[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_smaller = curr_smaller.assign(Y_PRED=curr_smaller_pred)\n",
    "                curr_bigger = curr_bigger.assign(Y_PRED=curr_bigger_pred)\n",
    "\n",
    "                for _, smaller_row in curr_smaller.iterrows():\n",
    "                    curr_expr_idx = smaller_row[expr_label]\n",
    "                    curr_diff = smaller_row[y_true_label] - smaller_row[y_pred_label]\n",
    "                    bigger_row = curr_bigger[curr_bigger[expr_label] == curr_expr_idx]\n",
    "                    assert(bigger_row.shape[0] == 1)\n",
    "                    bigger_row = bigger_row.iloc[0]\n",
    "                    new_y_true.append(bigger_row[y_true_label])\n",
    "                    new_y_pred.append(bigger_row[y_pred_label] + curr_diff)\n",
    "                new_preds.append(new_y_pred)\n",
    "            \n",
    "            curr_bigger = curr_bigger.assign(Y_PRED=np.mean(np.array(new_preds), axis=0))\n",
    "            df = pd.merge(curr_smaller, curr_bigger, on=[expr_label], suffixes=suffix_labels)\n",
    "            dfs.append(df)\n",
    "            \n",
    "#             new_y_true = np.array(new_y_true)\n",
    "#             new_y_pred = np.array(new_y_pred) \n",
    "            \n",
    "#             # score = rmse_score(new_y_true, new_y_pred)/(np.max(new_y_true) - np.min(new_y_true))\n",
    "#             score = nrmse_score(new_y_true, new_y_pred)\n",
    "            score = nrmse_score(df['Y_TRUE_large'].to_numpy(), df['Y_PRED_large'].to_numpy())\n",
    "\n",
    "            test_scores.append(score)\n",
    "                \n",
    "            models.append(model_dict[(cpu_a, cpu_b)])\n",
    "            datas_small.append(curr_smaller)\n",
    "            datas_big.append(curr_bigger)\n",
    "\n",
    "    print(test_scores)\n",
    "    if plot:\n",
    "        plot_prediction(dfs, X, y_true, expr_idxs, method)\n",
    "    # print(np.max(y) - np.min(y))\n",
    "    # overall_score = np.sqrt((np.array(test_scores)**2).mean())/(np.max(y) - np.min(y))\n",
    "    overall_score = np.mean(test_scores)\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd62e6d3-8c48-4381-b3b8-f3457ad4507d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use a dictionary of models\n",
    "- key: (lower SKU, higher SKU)\n",
    "- value: model\n",
    "'''\n",
    "def predict_pair(model_dicts, sku_pair, X, y_true, expr_idxs, method, plot=True):\n",
    "    overall_data = pd.DataFrame(zip(expr_idxs, y_true), columns=[expr_label, y_true_label])\n",
    "    overall_data[X_label] = X\n",
    "\n",
    "    # for each pair of SKU\n",
    "    num_cpus = np.sort(np.unique(X))\n",
    "    test_scores = []\n",
    "    models, datas_big, datas_small = [], [], []\n",
    "    dfs = []\n",
    "    \n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i != sku_pair[0] or j != sku_pair[1]:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            \n",
    "            curr_smaller = overall_data[overall_data[X_label] == cpu_a]\n",
    "            curr_bigger = overall_data[overall_data[X_label] == cpu_b]\n",
    "            \n",
    "            new_preds = []\n",
    "            \n",
    "            for model_dict in model_dicts:            \n",
    "                new_y_true, new_y_pred = [], []\n",
    "\n",
    "                curr_smaller_pred = model_dict[(cpu_a, cpu_b)].predict(curr_smaller[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_bigger_pred = model_dict[(cpu_a, cpu_b)].predict(curr_bigger[X_label].to_numpy().reshape(-1, 1))\n",
    "                curr_smaller = curr_smaller.assign(Y_PRED=curr_smaller_pred)\n",
    "                curr_bigger = curr_bigger.assign(Y_PRED=curr_bigger_pred)\n",
    "\n",
    "                for _, smaller_row in curr_smaller.iterrows():\n",
    "                    curr_expr_idx = smaller_row[expr_label]\n",
    "                    curr_diff = smaller_row[y_true_label] - smaller_row[y_pred_label]\n",
    "                    bigger_row = curr_bigger[curr_bigger[expr_label] == curr_expr_idx]\n",
    "                    assert(bigger_row.shape[0] == 1)\n",
    "                    bigger_row = bigger_row.iloc[0]\n",
    "                    new_y_true.append(bigger_row[y_true_label])\n",
    "                    new_y_pred.append(bigger_row[y_pred_label] + curr_diff)\n",
    "                new_preds.append(new_y_pred)\n",
    "\n",
    "            curr_bigger = curr_bigger.assign(Y_PRED=np.mean(np.array(new_preds), axis=0))\n",
    "            df = pd.merge(curr_smaller, curr_bigger, on=[expr_label], suffixes=suffix_labels)\n",
    "            dfs.append(df)\n",
    "            \n",
    "            score = nrmse_score(df['Y_TRUE_large'].to_numpy(), df['Y_PRED_large'].to_numpy())\n",
    "            test_scores.append(score)\n",
    "                \n",
    "            models.append(model_dict[(cpu_a, cpu_b)])\n",
    "            datas_small.append(curr_smaller)\n",
    "            datas_big.append(curr_bigger)\n",
    "            \n",
    "    print(test_scores)\n",
    "\n",
    "    if plot:        \n",
    "        plot_prediction(dfs, X, y_true, expr_idxs, method)\n",
    "    overall_score = np.mean(test_scores)\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4671fa42-e34d-4ed8-ab47-38603494895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# ycsb_X = get_cpu_nums_as_X(ycsb_data.cpu_nums)\n",
    "ycsb_X = get_data_X(ycsb_data)\n",
    "ycsb_y = np.array(ycsb_data.wl_throughput)\n",
    "ycsb_expr = np.array(ycsb_data.sampled_run_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56deaad-b448-492b-ac93-85d28bf1ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "all_times = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00318153-e6bc-457a-8723-c789810864bb",
   "metadata": {},
   "source": [
    "## NN\n",
    "\n",
    "Use only CPU number and target (latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4fb0f65-42ff-4b7b-91eb-3379ad6aa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# X_label = 'cpu_num'\n",
    "y_label = 'latency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f301f1b1-1168-4faa-87b3-d4c4dd10671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_model_indi(X, y, skus, expr_idxs, wl_name, grouping_type, groupping_id, plot=True): \n",
    "    train_scores, test_scores = [], []\n",
    "\n",
    "    run_label = 'RUN_IDX'\n",
    "    sku_vals = [int(e[3:]) for e in skus]\n",
    "    overall_data = pd.DataFrame(zip(sku_vals, expr_idxs, y), columns=['CPU_NUM', run_label, y_label])\n",
    "    overall_data[X_label] = X\n",
    "    # print(overall_data)\n",
    "    \n",
    "    # print(overall_data['CPU_NUM'])\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    uq_idx = np.unique(expr_idxs)\n",
    "    num_cpus = np.sort(np.unique(sku_vals))\n",
    "\n",
    "    models, datas = [], []\n",
    "    \n",
    "    model_dict = {}\n",
    "    train_time = 0\n",
    "\n",
    "    for i in range(len(num_cpus)):\n",
    "        for j in range(i, len(num_cpus)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cpu_a = num_cpus[i] # smaller\n",
    "            cpu_b = num_cpus[j] # larger\n",
    "            curr_data = overall_data[ (overall_data['CPU_NUM'] == cpu_a) | (overall_data['CPU_NUM'] == cpu_b) ]\n",
    "            # print(overall_data['CPU_NUM'])\n",
    "            # print(overall_data['CPU_NUM'] == cpu_a)\n",
    "            # print(curr_data)\n",
    "            datas.append(curr_data)\n",
    "            start = time.time()\n",
    "            # select by sampled run idx \n",
    "            for train_run_idxs, test_run_idxs in kf.split(uq_idx):\n",
    "                train_runs = uq_idx[train_run_idxs]\n",
    "                test_runs = uq_idx[test_run_idxs]\n",
    "                train_index = [i for i, value in enumerate(curr_data[run_label].to_list()) if value in train_runs]\n",
    "                test_index = [i for i, value in enumerate(curr_data[run_label].to_list()) if value in test_runs]\n",
    "                # train_index = curr_data[run_label] in train_runs\n",
    "                # test_index = curr_data[run_label] in test_runs\n",
    "                \n",
    "                data = curr_data.iloc[train_index]\n",
    "                test = curr_data.iloc[test_index]\n",
    "                \n",
    "                model = MLPRegressor(\n",
    "                    # hidden_layer_sizes=(10,10, 5,),  \n",
    "                    hidden_layer_sizes=(200, 150, 100, 50, 10, 5),  \n",
    "                    # hidden_layer_sizes=(10, 5),  \n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    # learning_rate='adaptive',\n",
    "                    random_state=42\n",
    "                )\n",
    "                train_x = np.array(data[X_label].to_list())\n",
    "                test_x = np.array(test[X_label].to_list())\n",
    "                model.fit(train_x, data[y_label])\n",
    "                test_pred = model.predict(test_x)\n",
    "                train_pred = model.predict(train_x)\n",
    "\n",
    "                score = nrmse_score(test[y_label], test_pred)\n",
    "                test_scores.append(score)\n",
    "                \n",
    "                score = nrmse_score(data[y_label], train_pred)\n",
    "                train_scores.append(score)\n",
    "            end = time.time()\n",
    "            train_time += (end - start) / k\n",
    "            \n",
    "            models.append(model)\n",
    "            model_dict[(cpu_a, cpu_b)] = model\n",
    "\n",
    "    overall_test = np.mean(test_scores)\n",
    "    overall_train = np.mean(train_scores)\n",
    "\n",
    "    return overall_test, overall_train, model_dict, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f11589-139d-4d11-a87d-c09e344d2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Id: 1, wl name tpcc, terminal num 32\n",
      "120\n",
      "Test rmse         = 1.0339189197825396, Train rmse         = 0.8090948249827713\n",
      "Group Id: 2, wl name tpch, terminal num 1\n",
      "360\n",
      "Test rmse         = 2.780864180156225, Train rmse         = 2.053326931233398\n",
      "Group Id: 3, wl name twitter, terminal num 32\n",
      "120\n",
      "Test rmse         = 7.694835777087804, Train rmse         = 5.494851994225933\n",
      "Group Id: 4, wl name twitter, terminal num 8\n",
      "120\n",
      "Test rmse         = 2.4647650627235453, Train rmse         = 1.9646967587373532\n",
      "Group Id: 7, wl name tpcc, terminal num 8\n",
      "120\n",
      "Test rmse         = 0.9129703731141264, Train rmse         = 0.7823197476512648\n",
      "Group Id: 8, wl name tpcc, terminal num 4\n",
      "120\n",
      "Test rmse         = 0.7344229705881057, Train rmse         = 0.56573561525881\n",
      "Group Id: 9, wl name twitter, terminal num 4\n",
      "120\n",
      "Test rmse         = 1.1996583796128537, Train rmse         = 0.9871210936852427\n",
      "Overall test nrmse: 2.4030622375807424; train 1.808163852253539\n"
     ]
    }
   ],
   "source": [
    "all_tests, all_trains = [], []\n",
    "nn_group_to_model_dict = {} \n",
    "all_results['NN'] = {}\n",
    "all_times['NN'] = {}\n",
    "for ty, curr_data in data_by_type.items():\n",
    "    name = curr_data.wl_names[0]\n",
    "    term = curr_data.terminal_num[0]\n",
    "    if name not in all_results['NN']:\n",
    "        all_results['NN'][name] = {}    \n",
    "        all_times['NN'][name] = {}\n",
    "\n",
    "    print(f'Group Id: {ty}, wl name {name}, terminal num {term}')\n",
    "    # X = get_cpu_nums_as_X(curr_data.cpu_nums)\n",
    "    X = get_data_X(curr_data)\n",
    "    y = np.array(curr_data.wl_throughput)\n",
    "\n",
    "    test_r2_mean, train_r2_mean, model_dict, train_time = build_nn_model_indi(X, y, curr_data.cpu_nums, np.array(curr_data.sampled_run_idx), name, 'group', curr_data.wl_groups[0], plot=OVERALL_PLOT)\n",
    "    nn_group_to_model_dict[ty] = model_dict\n",
    "    print('Test rmse         = {}, Train rmse         = {}'.format(test_r2_mean, train_r2_mean)) \n",
    "    all_tests.append(test_r2_mean)\n",
    "    all_trains.append(train_r2_mean)\n",
    "    all_results['NN'][name][term] = test_r2_mean\n",
    "    all_times['NN'][name][term] = train_time\n",
    "\n",
    "print('Overall test nrmse: {}; train {}'.format(np.mean(all_tests), np.mean(all_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e5dd482-5eb6-4b56-906a-f4666c4138f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&\\hlrfive{NN} & \\hlrfive{1.6856} &  \\hlrfive{0.734} & \\hlrfive{0.913} & \\hlrfive{1.034} & \\hlrfive{1.200} & \\hlrfive{2.465} & \\hlrfive{7.695} & \\hlrfive{2.781} &\\hlrfive{2.403} \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "terminals = [4, 8, 32]\n",
    "workloads = ['tpcc', 'twitter', 'tpch']\n",
    "methods = ['NN']\n",
    "\n",
    "for me in methods:\n",
    "    curr_line = ''\n",
    "    scs = []\n",
    "    times = []\n",
    "    for wl in workloads:\n",
    "        if wl == 'tpch':\n",
    "            sc = abs(all_results[me][wl][1])\n",
    "            curr_line += ' \\hlrfive{'\n",
    "            curr_line += f'{sc:.3f}'\n",
    "            curr_line += '} &'\n",
    "            scs.append(sc)\n",
    "            times.append(all_times[me][wl][1])\n",
    "        else:\n",
    "            for ter in terminals:\n",
    "                sc = abs(all_results[me][wl][ter])\n",
    "                curr_line += ' \\hlrfive{'\n",
    "                curr_line += f'{sc:.3f}'\n",
    "                curr_line += '} &'\n",
    "                scs.append(sc)\n",
    "                times.append(all_times[me][wl][ter])\n",
    "    curr_line += '\\hlrfive{' + f'{np.mean(scs):.3f}'+'}'\n",
    "    # curr_line += \" \\\\\\\\ \\cline{2-11}\"\n",
    "    curr_line += \" \\\\\\\\ \\hline\"\n",
    "    curr_line = '&\\hlrfive{' + f'{me}' + '} & \\hlrfive{' + f'{np.mean(times):.4f}' + '} & ' + curr_line\n",
    "    print(curr_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88c4d806-04e4-4de9-8f9f-285d4ec6bdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& NN &  1.6856 &  0.734 & 0.913 & 1.034 & 1.200 & 2.465 & 7.695 & 2.781 & 1.678 \\\\ \\cline{2-11}\n"
     ]
    }
   ],
   "source": [
    "for me in methods:\n",
    "    curr_line = ''\n",
    "    scs = []\n",
    "    times = []\n",
    "    for wl in workloads:\n",
    "        if wl == 'tpch':\n",
    "            sc = abs(all_results[me][wl][1])\n",
    "            curr_line += f' {sc:.3f} &'\n",
    "            scs.append(sc)\n",
    "            times.append(all_times[me][wl][1])\n",
    "        else:\n",
    "            for ter in terminals:\n",
    "                sc = abs(all_results[me][wl][ter])\n",
    "                curr_line += f' {sc:.3f} &'\n",
    "                scs.append(sc)\n",
    "                times.append(all_times[me][wl][ter])\n",
    "    scs = np.array(scs)\n",
    "    mask = np.logical_or(scs == scs.max(), scs == scs.min())\n",
    "    \n",
    "    a_masked = np.ma.masked_array(scs, mask = mask)\n",
    "    # print(scs, mask)\n",
    "\n",
    "    curr_line += f' {np.mean(a_masked):.3f}'\n",
    "    curr_line += \" \\\\\\\\ \\cline{2-11}\"\n",
    "    curr_line = f'& {me} & ' + f' {np.mean(times):.4f} & ' + curr_line\n",
    "    print(curr_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03f37c0b-87e2-4a87-be44-1aeca2542b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_model(X, y, skus, wl_name, grouping_type, groupping_id, plot=False, show_fig=False):  \n",
    "    train_rmses, test_rmses = [], []\n",
    "    k = 5\n",
    "    sku_vals = [int(e[3:]) for e in skus]\n",
    "    num_cpus = np.sort(np.unique(sku_vals))\n",
    "    train_time = 0\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        start = time.time()\n",
    "        # X_train = X[train_index]\n",
    "        X_train = np.array([X[idx] for idx in train_index ])\n",
    "        y_train = y[train_index]\n",
    "        # X_test = X[test_index]\n",
    "        X_test = np.array([X[idx] for idx in test_index])\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "        sku_train = [sku_vals[idx] for idx in train_index]\n",
    "        sku_test = [sku_vals[idx] for idx in test_index]\n",
    "        \n",
    "        model = MLPRegressor(\n",
    "            # hidden_layer_sizes=(10,10, 5,),  \n",
    "            hidden_layer_sizes=(200, 150, 100, 50, 10, 5),  \n",
    "            # hidden_layer_sizes=(10, 5),  \n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            # learning_rate='adaptive',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        train_pred = model.predict(X_train)\n",
    "        end = time.time()\n",
    "        train_time += end - start\n",
    "        for i in range(len(num_cpus)):\n",
    "            for j in range(i, len(num_cpus)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                cpu_a = num_cpus[i] # smaller\n",
    "                cpu_b = num_cpus[j] # larger\n",
    "                curr_mask = [x_lab == cpu_a or x_lab == cpu_b for x_lab in sku_test]\n",
    "                curr_y_true = y_test[curr_mask]\n",
    "                # rmse = np.sqrt(((curr_y_true-test_pred[curr_mask])**2).mean())\n",
    "                # n_rmse = rmse / (np.max(curr_y_true)-np.min(curr_y_true))\n",
    "                n_rmse = nrmse_score(curr_y_true, test_pred[curr_mask])\n",
    "\n",
    "                test_rmses.append(n_rmse)\n",
    "                \n",
    "                curr_mask = [x_lab == cpu_a or x_lab == cpu_b for x_lab in sku_train]\n",
    "                \n",
    "                curr_y_true = y_train[curr_mask]\n",
    "                # rmse = np.sqrt(((curr_y_true-train_pred[curr_mask])**2).mean())\n",
    "                # n_rmse = rmse / (np.max(curr_y_true)-np.min(curr_y_true))\n",
    "                n_rmse = nrmse_score(curr_y_true, train_pred[curr_mask])\n",
    "                train_rmses.append(n_rmse)\n",
    "    \n",
    "    train_time /= k\n",
    "\n",
    "    overall_test_rmse = np.mean(test_rmses)\n",
    "    overall_train_rmse = np.mean(train_rmses)\n",
    "    return overall_test_rmse, overall_train_rmse, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0ff3062-305c-4b10-bc98-6ed94c1940fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Id: 1, wl name tpcc, terminal num 32\n",
      "120\n",
      "Test rmse = 1.151271851064403, Train rmse = 0.9120665863599642\n",
      "Group Id: 2, wl name tpch, terminal num 1\n",
      "360\n",
      "Test rmse = 1.9711711426301748, Train rmse = 1.5989147860276052\n",
      "Group Id: 3, wl name twitter, terminal num 32\n",
      "120\n",
      "Test rmse = 8.547463251953733, Train rmse = 6.071638251508191\n",
      "Group Id: 4, wl name twitter, terminal num 8\n",
      "120\n",
      "Test rmse = 2.7681984444728753, Train rmse = 2.2308456716258886\n",
      "Group Id: 7, wl name tpcc, terminal num 8\n",
      "120\n",
      "Test rmse = 0.9312639400243554, Train rmse = 0.783652884204318\n",
      "Group Id: 8, wl name tpcc, terminal num 4\n",
      "120\n",
      "Test rmse = 0.6665004307422552, Train rmse = 0.5401193829927874\n",
      "Group Id: 9, wl name twitter, terminal num 4\n",
      "120\n",
      "Test rmse = 1.1848947923087623, Train rmse = 1.0424141831497642\n",
      "Overall test nrmse: 2.460109121885222, train 1.8828073922669313\n"
     ]
    }
   ],
   "source": [
    "all_tests, all_trains = [], []\n",
    "all_results['nn'] = {}\n",
    "all_times['nn'] = {}\n",
    "\n",
    "for ty, curr_data in data_by_type.items():\n",
    "    name = curr_data.wl_names[0]\n",
    "    term = curr_data.terminal_num[0]\n",
    "    if name not in all_results['nn']:\n",
    "        all_results['nn'][name] = {}\n",
    "        all_times['nn'][name] = {}\n",
    "    print(f'Group Id: {ty}, wl name {name}, terminal num {term}')\n",
    "    \n",
    "    X = get_data_X(curr_data)\n",
    "    y = np.array(curr_data.wl_throughput)\n",
    "    test_r2_mean, train_r2_mean, train_time = build_nn_model(X, y, curr_data.cpu_nums,\n",
    "                                                                     curr_data.wl_names[0], 'group', \n",
    "                                                                     curr_data.wl_groups[0], \n",
    "                                                                     plot=OVERALL_PLOT, show_fig=False)\n",
    "    print('Test rmse = {}, Train rmse = {}'.format(test_r2_mean, train_r2_mean))\n",
    "    all_tests.append(test_r2_mean)\n",
    "    all_trains.append(train_r2_mean)\n",
    "    all_results['nn'][name][term] = test_r2_mean\n",
    "    all_times['nn'][name][term] = train_time\n",
    "print('Overall test nrmse: {}, train {}'.format(np.mean(all_tests), np.mean(all_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2348104f-e1be-49ed-a35d-d9e80e2c6828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&\\hlrfive{nn} & \\hlrfive{0.3045} &  \\hlrfive{0.667} & \\hlrfive{0.931} & \\hlrfive{1.151} & \\hlrfive{1.185} & \\hlrfive{2.768} & \\hlrfive{8.547} & \\hlrfive{1.971} &\\hlrfive{2.460} \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "terminals = [4, 8, 32]\n",
    "workloads = ['tpcc', 'twitter', 'tpch']\n",
    "methods = ['nn']\n",
    "\n",
    "for me in methods:\n",
    "    curr_line = ''\n",
    "    scs = []\n",
    "    times = []\n",
    "    for wl in workloads:\n",
    "        if wl == 'tpch':\n",
    "            sc = abs(all_results[me][wl][1])\n",
    "            curr_line += ' \\hlrfive{'\n",
    "            curr_line += f'{sc:.3f}'\n",
    "            curr_line += '} &'\n",
    "            scs.append(sc)\n",
    "            times.append(all_times[me][wl][1])\n",
    "        else:\n",
    "            for ter in terminals:\n",
    "                sc = abs(all_results[me][wl][ter])\n",
    "                curr_line += ' \\hlrfive{'\n",
    "                curr_line += f'{sc:.3f}'\n",
    "                curr_line += '} &'\n",
    "                scs.append(sc)\n",
    "                times.append(all_times[me][wl][ter])\n",
    "    curr_line += '\\hlrfive{' + f'{np.mean(scs):.3f}'+'}'\n",
    "    # curr_line += \" \\\\\\\\ \\cline{2-11}\"\n",
    "    curr_line += \" \\\\\\\\ \\hline\"\n",
    "    curr_line = '&\\hlrfive{' + f'{me}' + '} & \\hlrfive{' + f'{np.mean(times):.4f}' + '} & ' + curr_line\n",
    "    print(curr_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8856b70e-254e-4d0e-8bd8-663c73f5bc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& NN &  1.6856 &  0.734 & 0.913 & 1.034 & 1.200 & 2.465 & 7.695 & 2.781 & 1.678 \\\\ \\cline{2-11}\n"
     ]
    }
   ],
   "source": [
    "for me in methods:\n",
    "    curr_line = ''\n",
    "    scs = []\n",
    "    times = []\n",
    "    for wl in workloads:\n",
    "        if wl == 'tpch':\n",
    "            sc = abs(all_results[me][wl][1])\n",
    "            curr_line += f' {sc:.3f} &'\n",
    "            scs.append(sc)\n",
    "            times.append(all_times[me][wl][1])\n",
    "        else:\n",
    "            for ter in terminals:\n",
    "                sc = abs(all_results[me][wl][ter])\n",
    "                curr_line += f' {sc:.3f} &'\n",
    "                scs.append(sc)\n",
    "                times.append(all_times[me][wl][ter])\n",
    "    scs = np.array(scs)\n",
    "    mask = np.logical_or(scs == scs.max(), scs == scs.min())\n",
    "    \n",
    "    a_masked = np.ma.masked_array(scs, mask = mask)\n",
    "    # print(scs, mask)\n",
    "\n",
    "    curr_line += f' {np.mean(a_masked):.3f}'\n",
    "    curr_line += \" \\\\\\\\ \\cline{2-11}\"\n",
    "    curr_line = f'& {me} & ' + f' {np.mean(times):.4f} & ' + curr_line\n",
    "    print(curr_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64220803-1a33-4e04-9084-3b91cc6d057b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workload_insights",
   "language": "python",
   "name": "workload_insights"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
